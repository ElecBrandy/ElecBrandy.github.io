+++
title = '[AI] 02. GPT'
date = 2024-12-31
featured_image = "https://wikibook.co.kr/images/cover/l/9791158395629.jpg"
tags = ['python', 'AI']
+++

{{<series title="📚 /한권으로 끝내는 LLM 파인튜닝" series="/2024_winter_llm">}}

<br>

## 1. 소개
____

> NLP: Natural Language Processing

2024 겨울방학을 맞아, 위키북스의 `한권으로 끝내는 LLM 파인튜닝` 스터디를 통해 지난 2023 데이터 청년 캠퍼스, 2024 AI 레드팀 컨퍼런스 등에서 맛보았던 LLM을 체험해보는 시간을 가져보기로 했다.

<br>
<br>

## 2. Runpod
____

### 2-1. 인공지능의 시작
결국 초기 단계에서 컴퓨팅 기술의 한께로 어려움을 겪었던 기계 번역 기술은 새로운 전환점을 맞이한다. 엘런 튜링은 Computing Machinery and Intelligence 논문에서 기계가 생각할수 있을까라는 질문을 던졌다. 이때 튜링의 개념은 현대 인공지능과 직접적으로 연결된다. 이 논문은 약 24000 이상 인용될 정도로 인공지능 분야에 많은 영향을 미쳤고 인공지능 연구의 시작점이 된다. **기게가 인간과 구별되지 않는 방식으로 행동할 수 있다면 그 기계는 '생각하고 있다'고 볼 수 있다고** 주장한 튜링. 이 주장을 바탕으로 설계한 실험이 바로 튜링 테스트 이다. 질문자, 응답자, 기계응답자가 참여하는 이 실험은, 만약 질문자가 기계를 인간으로 착각하게 할정도로 기계가 인간과 유사하게 응답할수있다면 그 기꼐가 튜링테스트를 통과했다고 간주함. 

### 2-2. 인공지능은 어떻게 학습하는가?
대체적으로 인공지능의 학습은 인간의 뇌를 모방하려는 노력에서 시작했다. 마치 뉴런처럼 . 1943 워런 맥컬록과 월터 ㅣ츠는 수학과 임계 논리를 기반으로 한 신경망 모델을 제안한다. 1과 0로 뉴런이 켜지고 꺼짐을 설정하는것. 이후 1949 도널드 헤브는 헤비안 학습이라는 세로운 이론을 제안함. 헤비안 학습은 함께 활동하는 뉴런들은 서로 더 강하게 연결된다라는 원리. 외부의 지시나 보상 없이도 뉴런들이 상호작용하면서 스스로 하습할 수 있다는 것. 사람의 행동과 습관과 비슷. 

19857년 프랭크 로젠블랫은 현재 인공신경망의 핵심이 되는 퍼셉트론을 개발한다. 인간의 뇌세포인 뉴런ㅇ을 모방해서 만들었음. 1958년 로젠블랫은 <The Precptorn: a probabilisitc model for information storage and oragniztion tin the brain> 이라는 제목의 논문을 발표함. 이 논문을 함께 한번 봐보자.

<br>
<br>

## 3. 퍼셉트론: 인공지능 학습의 첫 걸음
___
<The Precptorn: a probabilisitc model for information storage and oragniztion tin the brain> s을 보자. 로젠블랫은 정보의 저장방식과 저장된 정보가 인식과 행동에 미치는 영향에 초점을 맞춘다. 그는 기존 정보저장 모델이 지나치게 단순하고 정적이라고 지적함. 그래서 사람의 시각 신경계를 기반으로 퍼셉트론을 구현했다. 이 구조에서 정보는 고정된 형태로 저장되어 필요할때마다동일한 방식으로 인출되는 코드화된 기억 방식이 아니라 황성화된 뉴런들 사이의 새로운 연결 또는 경로를 통해 저장되며 이는 뉴런간의 연결 강도로 표현된다.  

이런 접근법으 ㅣ특징은 특정 자극과 반응 사이의 확률적 관계를 학습한다는 점이다. 그리고 연결이 경험에 따라 변하는 것임. 또한 입역치 이상의 자극이 왔을때 활성화되는 현상을 발견해 적용했으며 즉, 입력 신호의 가중합이 임곗값을 넘을 경우 뉴런이 활성화되는 방식으로 처리했다. 

예시로 '밥을' 이라는 입력이 들어오면 밥을과자주 사용된 단어가 서로 활성화되고 , 연관성이 없는 단어는 억제되는 것이다. 로젠블ㄹ렛은 이러한 현상을 자발적 조직화라고 명명했다. 그는 이 개념을 실험적으로 증명하려고 퍼셉트론 시스템에 두가지 서로 다른 유형의 자극을 무작위로 주는 실험을 했는데 이때 퍼셉트론은 이 두 유형의 자극을 스스로 구분했고 이를 통해 선형적 분리하는 중요한 개념을 발견함

선형적 분리란, 즉 퍼셉트론 시스템이 두 종류의 입력을 구분할 수 있는 능력을 말하는 것. 주어진 데이터를 직선이나 평면 형태로 구분가능하다는 것이지. 자극에는1, 다른자극에는 0 등등으로.

그러나 로젠블랫은 논문에 퍼셉트론의 다섯가지 주요 한계점을 발해놧음.
1. 인공지능 발전을 위해 새로운 접근 방식이 필요함. 단순한 개선으로는 불충분하고 근본적으로 다른 원칙이 필요함
2. 퍼셉트론 모델은 시간적 요소를 고려하지 않았음. 시간축이 없는 것은 한계가 크다.
3. 퍼셉트론 은 상대적 판단과 관계의 추상화에서 한계를 보인다고 설명. 단순한 패턴인식과 분류는 가능하지만 두 자극 간의 관계를 인식하는데 어려움이 있다고 봣음.
4. 퍼셉트론이 선형적으로 분리 가능한 문제만 해결 가능하다는 것. XOR와 같이 비선형적 분류 문제를 해결하지 못한다는 것을 의미


## 4. 역전파 알고리즘: 학습의 혁명
___
그렇다면 우리는 선형성 문제를 어떻게 해결해야할까. 그 전에 비선형성 개념을 한번 보자. 인공지능에서 선형 비선형은 단순히 직선과 곡선 문제가 아니라 얼마나 복잡한 태펀을 학습하고 문제를 해결할 수 있는지에 대한 문제이다. 즉, 비선형성은 입력과 출력 사이의 관계가 단순한 비례관계를 벗어나는 특성을 말하는 것. 작읍 입력 큰 출력변화 혹은 그 반대가 가능할 수도 있음.

단일 퍼셉트론 이후 연구자들은 한계극복을 위해 다층 퍼셉트론 구조를 제안함. 초기 인공신경망 연구에서는 주로 순방향을 ㅗ학습을 진행함. 그러나 역방향 전파를 연구하며 문제점이 드러남
예를들어 선형함수 f(x) = ax + b 를 미분하면 상수항은 b는 사라짐. 다시 또 미분하면 상수항 a도 사라짐. 이걸 역 방향 전파 과정을 거치면 결국 상수는 0이 됨. 그래서 연구자들은 비선형함수를 도입해서 이러한 문제를 방지하고 가중치를 효과적으로 업데이트하게함. 덕분에 신경망은 복잡성과 유연성을 가지게 되엇음.

그래서 역전파 알고리즘은 무엇이냐? 1986년 <learning Representations by Back-Propagating Errors> 논문은, 퍼셉트론 한계를 극복하고 비선형 문제를 해결할 수 있는 방법을 새로 제시함. 역전파 알고리즘은 아이에게 그림 그리는 법을 가르치는 것과 비슷함. 먼저 신경망은 입력을 받아 예측함. 그리고 예측과 실제 정답 사이 오차를 계산함. 이 오차를 출력층에서 시작해 입력층 방향으로 거꾸로 전파하면서 각 층의 가중치를 조정. 즉, 아이의 그림에 대해 구체적인 피드백을 주는 것.

이 과정에서 비선형 화럿ㅇ화 함수가 중요한 역할을 함. 

> 부록 참고해서 상세 내용 업데이트 하기

## 5. 트랜스포머의 등장: NLP의 새로운 시대
___
2013년 wrod2Vec이라는 단어 임베딩 기술이 등장했음. 이건 데이터 청년캠퍼스떄 주구장창 썼던 것인데 이렇게 보니 슬프기도 하다. 그때 이런 내용을 좀 미리 공부했다면 더 좋은 프롲게트가 가능했을지도..? 어쨌ㄷ느든 이후 2014 GLOVE, rNN, 2015 어텐션, 2017 트랜스vhaj ahepf, 2018 BERT, GPT와같은 사전 훈련된 언어 모델의 등장으로 이어졌다.  

다음 시간에는 GPT에 대해서 알아보자.


## 7. Reference
____
- https://product.kyobobook.co.kr/detail/S000214934825
- https://wikibook.co.kr/

<br>
<br>